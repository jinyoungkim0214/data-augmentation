{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4PD4+JiuAQ2rtFGDYoNSC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinyoungkim0214/data-augmentation/blob/main/data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3E30qV2-CX5"
      },
      "outputs": [],
      "source": [
        "transforms.RandomResizedCrop(224),           # 무작위 크롭 후 224x224\n",
        "transforms.RandomHorizontalFlip(p=0.5),      # 50% 확률로 수평 뒤집기\n",
        "transforms.RandomVerticalFlip(p=0.5),        # 50% 확률로 수직 뒤집기\n",
        "transforms.RandomRotation(degrees=30),       # 최대 30도 무작위 회전\n",
        "transforms.ColorJitter(brightness=0.2,       # 밝기, 대비, 채도, 색조 조정\n",
        "                                   contrast=0.2,\n",
        "                                   saturation=0.2,\n",
        "                                   hue=0.1),\n",
        "transforms.RandomAffine(degrees=0,            # 이동 및 스케일링\n",
        "                                   translate=(0.1, 0.1),\n",
        "                                   scale=(0.8, 1.2)),\n",
        "transforms.RandomErasing(p=0.3,              # 30% 확률로 무작위 영역 지우기\n",
        "                                    scale=(0.02, 0.2),\n",
        "                                    ratio=(0.3, 3.3)),"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "materials classification 성능 향상시킬 만한 방법들\n",
        "\n",
        "materials classification 파일에는 random resized crop만 됨 -> augmentation 코드 하나씩 넣어보면서 성능 비교해볼 수 있음\n",
        "\n",
        "\n",
        "activation function 바꾸거나, 다른 network embedding 시키거나 하는 방법도 적용 가능\n",
        "\n",
        "SGD를 아담이나 아담W로 바꿔보기 가능\n",
        "\n",
        "아담:\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "아담W:\n",
        "optimizer = optim.AdamW(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "Cross entropy loss 말고 다른 loss 추가(재료적 특성 반영할 만한 새로운 loss 설계)\n",
        "class 나눠놓고 아예 잘못 대분류하면 loss를 크게 준다거나."
      ],
      "metadata": {
        "id": "RJTMZyrQEC6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 스타일 손실 계산 함수 #새로운 loss를 추가하는 방식이긴 한데, 훨씬 정교하게 짤 필요는 있어 보임.\n",
        "def gram_matrix(feature_map):\n",
        "    \"\"\"특징 맵의 Gram 행렬을 계산\"\"\"\n",
        "    b, c, h, w = feature_map.size()  # 배치, 채널, 높이, 너비\n",
        "    features = feature_map.view(b, c, h * w)  # [b, c, h*w]\n",
        "    gram = torch.bmm(features, features.transpose(1, 2))  # [b, c, c]\n",
        "    return gram / (c * h * w)  # 정규화\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.mse_loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, input_features, target_features):\n",
        "        \"\"\"입력과 타겟 특징 맵의 스타일 손실 계산\"\"\"\n",
        "        input_gram = gram_matrix(input_features)\n",
        "        target_gram = gram_matrix(target_features)\n",
        "        return self.mse_loss(input_gram, target_gram)\n",
        "\n",
        "# 모델 수정: 중간 특징 맵 추출\n",
        "class BaselineModel1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fe = ResNetFeatureExtractor(resnet_type=\"resnet101\", in_channels=3)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(2048, 47)\n",
        "\n",
        "        # 스타일 손실을 위한 중간 레이어 저장\n",
        "        self.intermediate_features = []\n",
        "        self.fe.feature_extractor.layer3.register_forward_hook(self._hook_fn)\n",
        "\n",
        "    def _hook_fn(self, module, input, output):\n",
        "        self.intermediate_features.append(output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.intermediate_features = []  # 초기화\n",
        "        x = self.fe(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x, self.intermediate_features\n",
        "\n",
        "# 손실 함수 정의\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "style_loss_fn = StyleLoss()\n",
        "\n",
        "# 옵티마이저 (AdamW, 이전 설정 유지)\n",
        "optimizer = optim.AdamW(net.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "# 손실 기록용 리스트\n",
        "training_loss_history = []\n",
        "test_loss_history = []\n",
        "\n",
        "# 훈련 루프\n",
        "net = BaselineModel1()\n",
        "style_weight = 1e4  # 스타일 손실 가중치 (조정 가능)\n",
        "for epoch in range(2):  # 2 epoch\n",
        "    # 학습 단계\n",
        "    running_loss = 0.0\n",
        "    running_style_loss = 0.0\n",
        "    net.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 모델 출력 (분류 출력 + 중간 특징 맵)\n",
        "        outputs, intermediate_features = net(inputs)\n",
        "\n",
        "        # 분류 손실\n",
        "        cls_loss = criterion(outputs, labels)\n",
        "\n",
        "        # 스타일 손실 (layer3의 특징 맵 사용)\n",
        "        if intermediate_features:\n",
        "            style_loss = style_loss_fn(intermediate_features[0], intermediate_features[0])  # 입력 자체와 비교\n",
        "            total_loss = cls_loss + style_weight * style_loss\n",
        "        else:\n",
        "            total_loss = cls_loss\n",
        "\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += cls_loss.item()\n",
        "        running_style_loss += style_loss.item() if intermediate_features else 0.0\n",
        "\n",
        "        if i % 10 == 9:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] cls_loss: {running_loss / 200:.3f}, style_loss: {running_style_loss / 200:.3f}')\n",
        "            training_loss_history.append(running_loss / 200)\n",
        "            running_loss = 0.0\n",
        "            running_style_loss = 0.0\n",
        "\n",
        "    # 테스트 단계\n",
        "    running_test_loss = 0.0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, test_data in enumerate(testloader, 0):\n",
        "            test_images, test_labels = test_data\n",
        "            test_outputs, _ = net(test_images)\n",
        "            test_loss = criterion(test_outputs, test_labels)\n",
        "            running_test_loss += test_loss.item()\n",
        "\n",
        "    avg_test_loss = running_test_loss / (i + 1)\n",
        "    test_loss_history.append(avg_test_loss)\n",
        "\n",
        "    # 학습률 스케줄러 업데이트\n",
        "    scheduler.step(avg_test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch + 1} - Test Loss: {avg_test_loss:.3f}')\n",
        "\n",
        "print('학습 끝!')\n",
        "```"
      ],
      "metadata": {
        "id": "pGjuFuvEGlI4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}